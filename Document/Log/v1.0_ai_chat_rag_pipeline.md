# v1.0 AI Chat — Energy Physics RAG Pipeline

**开发日期**: 2026-02-22

---

## 开发概述

将原有的 AI Chat 模拟占位符（固定回复字符串 + 1.5s 超时）替换为一套完整的纯前端 RAG（检索增强生成）问答系统，限定范围为 Energy Physics 领域，直接部署在 GitHub Pages 上，不依赖任何后端服务。

---

## 新增文件

| 文件 | 功能 |
|---|---|
| `docs/js/knowledge-index.js` | 从 `material.json` 提取文本 Chunk，构建可检索的知识库索引 |
| `docs/js/scope-guard.js` | 基于关键词分类器，快速判断查询是否属于 Energy Physics 领域 |
| `docs/js/embeddings.js` | TF-IDF 向量化 + 可选 Transformers.js 神经嵌入（双路混合检索） |
| `docs/js/retriever.js` | 混合检索编排器，返回 top-k 最相关证据 Chunk |
| `docs/js/llm-client.js` | OpenAI-compatible REST 客户端（可通过 baseURL 切换到任何兼容服务商） |
| `docs/js/prompt-policy.js` | 系统提示词构建 + 证据注入格式 + 拒答模板 |
| `docs/js/response-guard.js` | LLM 输出验证：提取引用 ID、过滤模型漂移、格式化 HTML |

---

## 修改文件

### `docs/js/ai-chat.js`
完全重写 `handleSend()` 函数，替换模拟逻辑为以下流水线：

```
用户输入
  → ScopeGuard.checkScope()          ← 域外直接拒答
  → Retriever.retrieve()              ← TF-IDF 混合检索，top-k 证据
  → LLMClient.chat(messages)          ← OpenAI-compatible API 调用
  → ResponseGuard.guard()             ← 提取引用，过滤漂移
  → addMessage(text, citations)       ← 渲染答案 + 引用来源列表
```

### `docs/index.html`
- 在 `ai-chat.js` 之前，按依赖顺序插入 7 个新模块的 `<script>` 标签
- 更新内嵌的 aiChat 配置（添加 `apiKey`, `baseURL`, `model`, `retrieval` 等字段）

### `docs/material.json` / `material.json`
在 `index.aiChat` 中新增运行时配置字段：

```json
{
  "provider": "openai",
  "baseURL": "https://api.openai.com/v1",
  "model": "gpt-4o-mini",
  "apiKey": "",
  "temperature": 0.3,
  "maxTokens": 800,
  "retrieval": {
    "topK": 4,
    "useEmbeddings": false,
    "minKeywordScore": 0.05
  }
}
```

---

## 实现方式

### 知识库构建（KnowledgeIndex）
- 从 material.json 以下 5 个 section 中提取内容 Chunk：
  - `highlights.items[]`（核裂变介绍）
  - `Fusion2.items[]`（核聚变）
  - `safety.items[]`（MSR 熔盐堆）
  - `features.cards[]`（可再生能源）
  - `sustainability.items[]`（化石燃料 vs 核能）
- 每个 Chunk 包含：`id`, `section`, `sectionName`, `title`, `text`（HTML 去标签），`refIds`（引用 ID 数组），`refs`（完整引用对象）
- 使用 DOM `DOMParser` 提取 `.ref-cite[data-ref-id]` 的引用 ID，保持与页面现有引用系统的一致性

### 混合检索（EmbeddingService + Retriever）
- **主路径（TF-IDF）**：构建 Chunk 的 TF-IDF 稀疏向量，计算查询与 Chunk 的余弦相似度，始终可用
- **可选路径（Neural）**：`useEmbeddings: true` 时，动态加载 `@xenova/transformers`（`Xenova/all-MiniLM-L6-v2`，约 25MB），失败自动回退到 TF-IDF
- 混合评分：Neural 权重 60% + TF-IDF 权重 40%
- 返回 `minScore` 以上的 top-k Chunk

### 领域守卫（ScopeGuard）
- 维护 Energy Physics 领域词汇表（约 80 个词组）
- 维护硬拒绝词汇表（约 30 个明显 OOD 词组）
- 支持 meta 查询（"你能做什么"）白名单
- 短查询（≤4词）使用宽松模式，长查询（>8词）且无领域词时拒答
- 输出 `{ inScope, confidence: 'high'|'medium'|'low', reason }`

### 系统提示词（PromptPolicy）
- 角色定义：Durham University Physics in Society 项目的核能教育助手
- 8 条严格规则：仅限 Energy Physics / 拒绝越界 / 证据优先 / 证据不足声明 / 引用格式 / 语言风格 / 长度控制 / LaTeX 支持
- 每次调用前注入 top-k 证据 Chunk，格式为带编号的文本块
- 系统消息 + 用户消息（包含 [EVIDENCE] 和 [STUDENT QUESTION] 两个段落）

### 输出验证（ResponseGuard）
- 检测模型自我介绍漂移（"I am ChatGPT" 等），截断并还原有用内容
- 从 "Sources: [2], [7]" 页脚和正文 `[N]` inline 引用中提取引用 ID
- 移除 Sources 页脚（改为专用引用区块渲染）
- 转义潜在危险 HTML 标签，换行转 `<br>`

### 引用渲染
- 在助手消息气泡内底部追加 "Sources" 区块
- 每条引用：显示 `[id] 引用文本（截断至 160 字符）`，若有 URL 则渲染为可点击链接

---

## 配置方式

在 `docs/material.json` 的 `index.aiChat` 中填写 API Key 即可启用：

```json
"apiKey": "sk-..."
```

- 默认模型：`gpt-4o-mini`（成本低，速度快）
- `baseURL` 可替换为任何 OpenAI-compatible 服务商（如 DeepSeek, Groq, Together.ai 等）
- 启用神经嵌入：`"retrieval": { "useEmbeddings": true }`（会额外加载约 25MB 模型）

---

## 注意事项

- API Key 保存在 `material.json` 中，对任何访问该仓库或网页的人可见；本项目为教育演示用途，已明确接受此安全取舍
- 纯前端系统提示词约束无法做到生产级可靠；如需更严格的领域限制，应增加后端策略层
- 当 API Key 未配置时，系统自动进入降级模式：ScopeGuard 和 Retriever 仍工作，显示检索到的相关章节列表，并提示用户配置 Key

---

## 与前版本的对比

| 项目 | v0.9.1 及之前 | v1.0 |
|---|---|---|
| AI Chat 实现 | 1.5s 超时返回固定字符串 | 完整 RAG 流水线 |
| 知识库 | 无 | 从 material.json 自动构建（约 20 Chunk）|
| 检索 | 无 | TF-IDF 余弦相似度（+ 可选神经嵌入）|
| 域外过滤 | 无 | ScopeGuard（关键词分类器）|
| 引用 | 无 | 自动提取 + 渲染引用链接 |
| LLM 接口 | 无 | OpenAI-compatible（可换供应商）|

---

## 增量更新：Streaming + 思考折叠（2026-02-22）

在 v1.0 的基础上新增了流式输出和折叠思考内容显示（C 方案）：

- `docs/js/llm-client.js`
  - 新增 `chatStream(messages, handlers)`，基于 SSE 读取 `chat/completions` 的流式返回。
  - 自动兼容常见推理字段：`reasoning`、`reasoning_content`、`thinking`。
  - 非流式 `chat()` 也补充了 reasoning 提取，保持行为一致。

- `docs/js/ai-chat.js`
  - 新增流式渲染：AI 回复会边接收边显示，不再等待整段返回。
  - 在最终回复下方新增折叠区：`Model thinking (raw)`，按用户要求原样显示思考文本。
  - 保留既有 `Sources` 引用区，与 reasoning 折叠区并存。

- 新增配置项（`index.aiChat`）：
  - `"stream": true`
  - `"showReasoningFold": true`
  - 已同步到 `docs/material.json`、`material.json`、`docs/index.html` 内嵌回退配置。

## 增量修复：AI Chat 无法发送（2026-02-22）

### 现象
- `docs` 页面与编辑器页面都出现“输入后无法发送/无响应”。

### 根因
- 发送按钮选择器依赖 `i[data-lucide=\"arrow-up\"]`，而 Lucide 初始化后会把 `<i>` 替换为 `<svg>`，导致 `sendBtn` 取不到，事件未绑定。
- 编辑器页面 `index.html` 仍在加载旧版 `./js/ai-chat.js`（模拟逻辑），未接入新的 RAG 模块链路。

### 修复
- `docs/js/ai-chat.js`：发送按钮改为多重兜底选择器（优先 `aria-label="Send message"`）。
- `docs/js/templates.js` 与 `js/templates.js`：给发送按钮增加 `aria-label="Send message"`，避免图标替换后失效。
- `index.html`（编辑器页）：切换为加载共享的新 AI 模块（`./docs/js/*`）与新版 `./docs/js/ai-chat.js`。

### 结果
- `docs` 导出页与前端编辑页共用同一套 AI Chat 管线：ScopeGuard → Retriever → LLMClient（支持流式）→ ResponseGuard。

## 增量修复：Cloudflare Worker 代理接入（2026-02-22）

### 背景
- 浏览器直连 `https://integrate.api.nvidia.com/v1/chat/completions` 被 CORS 拦截，导致前端无法发送请求。
- 用户已提供可用 Worker 端点：`https://envidia.dreamwithchangan.workers.dev/api/chat`。

### 验证
- 使用命令行对 Worker 进行了两类验证：
  - 非流式 `POST` 正常返回 `chat.completion` JSON；
  - `stream: true` 返回 `text/event-stream`，包含 `delta.content` 与 `delta.reasoning/reasoning_content`。

### 代码改动
- `docs/js/llm-client.js`
  - 新增可配置 `endpointPath`（默认 `/chat/completions`）。
  - 新增 `allowNoApiKey` 与 `sendAuthorizationHeader`，支持“前端不带 Key，后端代理持有 Key”模式。
  - 请求端点由硬编码改为 `baseURL + endpointPath`。

- `docs/js/ai-chat.js`
  - 将新增配置项透传至 `LLMClient.init()`：
    - `endpointPath`
    - `allowNoApiKey`
    - `sendAuthorizationHeader`

- 配置同步
  - `docs/material.json`、`material.json`、`docs/index.html` 的 `index.aiChat` 已统一为 Worker 模式：
    - `"baseURL": "https://envidia.dreamwithchangan.workers.dev"`
    - `"endpointPath": "/api/chat"`
    - `"allowNoApiKey": true`
    - `"sendAuthorizationHeader": false`
    - `"apiKey": ""`

### 结果
- GitHub Pages 前端可继续保持静态部署；
- 通过 Worker 代理规避浏览器 CORS；
- 仍保留 streaming 与 reasoning 折叠显示能力。

## 增量优化：公式渲染与引用跳转（2026-02-22）

- `docs/js/ai-chat.js`
  - AI 回答渲染后自动调用 `LatexRenderer.renderElement(...)`，使聊天中的 LaTeX 公式可视化（KaTeX）。
  - Streaming 模式下，在最终内容落地时也会触发公式渲染。
  - Sources 区引用行为更新为“优先跳转页面底部 Reference 条目”：
    - 每条引用生成 `ref-cite` 链接并携带 `data-ref-id`，复用现有滚动高亮逻辑；
    - 若引用含 URL（或文本中可提取 URL），同时提供 `[open]` 外链入口。

## 增量优化：公式漏渲染修复 + 章节引用（2026-02-22）

- `docs/js/ai-chat.js`
  - 增加 `renderMathWithRetry()`，在消息渲染后做多次 KaTeX 重试，并调用 `LatexRenderer.renderAll()` 兜底，解决“偶发不渲染”。
  - Sources 区新增 `Sections` 分组，展示本次回答命中的章节（来自检索证据），支持点击跳转到对应 section。

- `docs/js/response-guard.js`
  - 新增 `sectionCitations` 输出（由 evidence chunks 去重汇总）。
  - 文本格式化新增 LaTeX 分隔符归一化（处理部分模型返回的双反斜杠分隔符）。

## 增量修复：References 长期仅出现 [1]（2026-02-22）

- 现象：模型偶尔只在输出里写 `Sources: [1]`，导致 UI 看起来“总是第一个参考文献”。
- 修复：`docs/js/response-guard.js` 的引用组装改为：
  - 模型显式引用（`Sources`/inline）；
  - + 检索证据 chunks 的引用（`refIds` 与 `refs`）；
  - 合并去重后展示（最多 8 条）。
- 结果：即使模型偷懒只给 `[1]`，前端仍会补全与当前回答相关的其它文献引用。

## 增量修复：References 仅首条可跳转（2026-02-22）

- 现象：References 列表里只有第一条带 `[id]` 的文献能跳转到底部 Reference，其它条目仅有 `[open]` 外链。
- 根因：许多检索证据来源（如卡片内 `references`）只有文本/URL，没有 footer `id`，UI 无法生成 `#ref-{id}` 跳转。
- 修复：`docs/js/response-guard.js` 新增“引用 ID 解析”逻辑：
  - 先按 URL 在 footer 引文文本中匹配；
  - 再按文本片段做兜底匹配；
  - 匹配成功后为该引用补齐 `id`，从而支持跳转。
- 结果：只要能匹配到 footer 中的对应文献，References 中该条目也会变成可点击跳转。

## 增量优化：AI Chat 放大/还原按钮（2026-02-22）

- 将 AI Chat 右上角图标从“缩小”语义改为“放大”语义（初始 `maximize-2`）。
- 新增可切换放大/还原逻辑（toggle）：
  - 放大高度目标为当前容器高度的 2 倍；
  - 为避免超出视口，增加上限保护（`92vh`）。
- 放大后按钮图标切换为 `minimize-2`，再次点击还原到原始高度。
